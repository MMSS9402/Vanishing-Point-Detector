{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from datetime import date\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_matterport_dataset\n",
    "from models import build_model\n",
    "from config import cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"jet\")\n",
    "norm = mpl.colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(x):0.2 M 3.0 86.3 90.3 15.8 24.6 36.0 61.7 73.6 84.4\n",
    "NeurVPS [69] 22 M 0.5 93.9 96.3 24.0 41.8 64.4 52.4 64.0 77.8\n",
    "Ours 7 M 5.5 84.0 90.2 24.8 42.1 63.7 60.7 74.3 86.3\n",
    "Ours∗ 5 M 23.0 84.8 90.7 22.9 39.8 62.4 59.5 72.6 85.4\n",
    "Ours† 7 M 5.5 81.7 88.7 22.2 38.8 59.9 59.1 72.6\n",
    "                        help=\"path to config file\",\n",
    "                        type=str,\n",
    "                        default='/home/kmuvcl/source/CTRL-C/config-files/ctrl-c.yaml')\n",
    "    parser.add_argument(\"--opts\",\n",
    "                        help=\"Modify config options using the command-line\",\n",
    "                        default=None,\n",
    "                        nargs=argparse.REMAINDER\n",
    "                        )\n",
    "    return parser\n",
    "\n",
    "def compute_vp_err(vp1, vp2, dim=-1):\n",
    "    cos_sim = F.cosine_similarity(vp1, vp2, dim=dim).abs()\n",
    "    cos_sim = np.clip(cos_sim.item(), 0.0, 1.0)    \n",
    "    return np.degrees(np.arccos(cos_sim))\n",
    "\n",
    "def compute_hl_np(hl, sz, eps=1e-6):\n",
    "    (a,b,c) = hl\n",
    "    if b < 0:\n",
    "        a, b, c = -a, -b, -c\n",
    "    b = np.maximum(b, eps)\n",
    "    \n",
    "    left = np.array([-1.0, (a - c)/b])        \n",
    "    right = np.array([1.0, (-a - c)/b])\n",
    "\n",
    "    # scale back to original image    \n",
    "    scale = sz[1]/2\n",
    "    left = scale*left\n",
    "    right = scale*right\n",
    "    return [np.squeeze(left), np.squeeze(right)]\n",
    "\n",
    "def compute_up_vector(zvp, fovy, eps=1e-7):\n",
    "    # image size 2 (-1~1)\n",
    "    focal = 1.0/np.tan(fovy/2.0)\n",
    "    \n",
    "    if zvp[2] < 0:\n",
    "        zvp = -zvp\n",
    "    zvp = zvp / np.maximum(zvp[2], eps)\n",
    "    zvp[2] = focal\n",
    "    return normalize_safe_np(zvp)\n",
    "\n",
    "def decompose_up_vector(v):\n",
    "    pitch = np.arcsin(v[2])\n",
    "    roll = np.arctan(-v[0]/v[1])\n",
    "    return pitch, roll\n",
    "\n",
    "def cosine_similarity(v1, v2, eps=1e-7):\n",
    "    v1 = v1 / np.maximum(LA.norm(v1), eps)\n",
    "    v2 = v2 / np.maximum(LA.norm(v2), eps)\n",
    "    return np.sum(v1*v2)\n",
    "\n",
    "def normalize_safe_np(v, eps=1e-7):\n",
    "    return v/np.maximum(LA.norm(v), eps)\n",
    "\n",
    "def compute_up_vector_error(pred_zvp, pred_fovy, target_up_vector):\n",
    "    pred_up_vector = compute_up_vector(pred_zvp, pred_fovy)\n",
    "    cos_sim = cosine_similarity(target_up_vector, pred_up_vector)\n",
    "\n",
    "    target_pitch, target_roll = decompose_up_vector(target_up_vector)\n",
    "\n",
    "    if cos_sim < 0:\n",
    "        pred_pitch, pred_roll = decompose_up_vector(-pred_up_vector)\n",
    "    else:\n",
    "        pred_pitch, pred_roll = decompose_up_vector(pred_up_vector)\n",
    "\n",
    "    err_angle = np.degrees(np.arccos(np.abs(cos_sim)))\n",
    "    err_pitch = np.degrees(np.abs(pred_pitch - target_pitch))\n",
    "    err_roll = np.degrees(np.abs(pred_roll - target_roll))\n",
    "    return err_angle, err_pitch, err_roll\n",
    "\n",
    "def compute_fovy_error(pred_fovy, target_fovy):\n",
    "    pred_fovy = np.degrees(pred_fovy)\n",
    "    target_fovy = np.degrees(target_fovy)\n",
    "    err_fovy = np.abs(pred_fovy - target_fovy)\n",
    "    return err_fovy.item()\n",
    "\n",
    "def compute_horizon_error(pred_hl, target_hl, img_sz):\n",
    "    target_hl_pts = compute_hl_np(target_hl, img_sz)\n",
    "    pred_hl_pts = compute_hl_np(pred_hl, img_sz)\n",
    "    err_hl = np.maximum(np.abs(target_hl_pts[0][1] - pred_hl_pts[0][1]),\n",
    "                        np.abs(target_hl_pts[1][1] - pred_hl_pts[1][1]))\n",
    "    err_hl /= img_sz[0] # height\n",
    "    return err_hl\n",
    "    \n",
    "def draw_attention(img, weights, cmap, savepath):\n",
    "    extent = [-1, 1, 1, -1]\n",
    "    num_layer = len(weights)\n",
    "    plt.figure(figsize=(num_layer*3,3))\n",
    "    for idx_l in range(num_layer):                    \n",
    "        plt.subplot(1, num_layer, idx_l + 1)\n",
    "        plt.imshow(img, extent=extent)\n",
    "        plt.imshow(weights[idx_l], cmap=cmap, alpha=0.3, extent=extent)\n",
    "        plt.axis('off')\n",
    "    plt.savefig(savepath, pad_inches=0, bbox_inches='tight')\n",
    "    plt.close('all')\n",
    "\n",
    "def draw_attention_segs(img, weights, segs, cmap, savepath):\n",
    "    num_layer = len(weights)\n",
    "    num_segs = len(segs)\n",
    "    plt.figure(figsize=(num_layer*3,3))\n",
    "    for idx_l in range(num_layer):                    \n",
    "        plt.subplot(1, num_layer, idx_l + 1)\n",
    "        plt.imshow(img, extent=[-1, 1, 1, -1])                 \n",
    "        ws = weights[idx_l]\n",
    "        ws = (ws - ws.min())/(ws.max() - ws.min())\n",
    "        for idx_s in range(num_segs):\n",
    "            plt.plot((segs[idx_s,0], segs[idx_s,2]), \n",
    "                     (segs[idx_s,1], segs[idx_s,3]), c=cmap(ws[idx_s]))\n",
    "        plt.axis('off')\n",
    "    plt.savefig(savepath, pad_inches=0, bbox_inches='tight')\n",
    "    plt.close('all')    \n",
    "    \n",
    "def to_device(data, device):\n",
    "    if type(data) == dict:\n",
    "        return {k: v.to(device) for k, v in data.items()}\n",
    "    return [{k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "             for k, v in t.items()} for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(cfg.DEVICE)\n",
    "model, _ = build_model(cfg)\n",
    "model.to(device)\n",
    "\n",
    "dataset_test = build_matterport_dataset(image_set='train', cfg=cfg)\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = DataLoader(dataset_test, 1, sampler=sampler_test,\n",
    "                                drop_last=False, \n",
    "                                collate_fn=utils.collate_fn, \n",
    "                                num_workers=2)\n",
    "\n",
    "output_dir = Path(cfg.OUTPUT_DIR)\n",
    "\n",
    "checkpoint = torch.load('/home/kmuvcl/source/CTRL-C/line_embed/checkpoint0099.pth', map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.eval()\n",
    "\n",
    "# initlaize for visualization\n",
    "name = f'gsv_test_{date.today()}'\n",
    "if cfg.TEST.DISPLAY:\n",
    "    fig_output_dir = osp.join(output_dir,'{}'.format(name))\n",
    "    os.makedirs(fig_output_dir, exist_ok=True)\n",
    "\n",
    "dict = {'vp_dot_product':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (samples, extra_samples, targets) in enumerate(tqdm(data_loader_test)):\n",
    "        with torch.no_grad():\n",
    "            samples = samples.to(device)\n",
    "            print(type(extra_samples))\n",
    "            extra_samples = to_device(extra_samples, device)\n",
    "            outputs, extra_info = model(samples, extra_samples)\n",
    "            img = targets[0]['org_img']\n",
    "        \n",
    "            pred_vp1 = outputs['pred_vp1'].to('cpu')[0].numpy()\n",
    "            pred_vp2 = outputs['pred_vp2'].to('cpu')[0].numpy()\n",
    "            pred_vp3 = outputs['pred_vp3'].to('cpu')[0].numpy()\n",
    "\n",
    "            print(pred_vp1,pred_vp2,pred_vp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmuvcl/anaconda3/envs/debug/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import argparse\n",
    "from datetime import date\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "import util.misc as utils\n",
    "from datasets import build_matterport_dataset\n",
    "from models import build_model\n",
    "from config import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if type(data) == dict:\n",
    "        return {k: v.to(device) for k, v in data.items()}\n",
    "    return [{k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "             for k, v in t.items()} for t in data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = build_model(cfg)\n",
    "dataset_test = build_matterport_dataset(image_set='test', cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = build_matterport_dataset(image_set='test', cfg=cfg)\n",
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = DataLoader(dataset_test, 1, sampler=sampler_test,\n",
    "                                drop_last=False, \n",
    "                                collate_fn=utils.collate_fn, \n",
    "                                num_workers=2)\n",
    "device = torch.device(cfg.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4180 [00:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (samples, extra_samples, targets) in enumerate(tqdm(data_loader_test)):\n",
    "    with torch.no_grad():\n",
    "        samples = samples.to(device)\n",
    "        extra_samples = to_device(extra_samples, device)\n",
    "        samples = samples\n",
    "        extra_samples = extra_samples\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1264, -0.1780,  0.8407,  ...,  1.0735,  0.2685, -1.8008],\n",
       "          [ 0.0891, -0.8786,  0.1789,  ...,  1.2273,  1.7090, -0.3862],\n",
       "          [-1.3127,  1.2338, -0.5457,  ..., -0.5208,  2.7156,  0.4870],\n",
       "          ...,\n",
       "          [-0.3699, -1.0791,  1.0718,  ...,  0.9229,  0.2116,  0.4318],\n",
       "          [ 2.1544, -0.7103, -0.0838,  ..., -0.6311,  1.7091,  0.7185],\n",
       "          [-0.2634,  0.5566,  0.1837,  ...,  1.1263,  1.2414,  1.8262]],\n",
       "\n",
       "         [[ 1.5707, -0.1305, -2.1983,  ..., -0.6357, -0.7531, -0.2143],\n",
       "          [ 0.9331,  0.9583, -2.4119,  ...,  0.4475, -0.2008, -0.7467],\n",
       "          [ 1.0358, -2.0862, -1.5219,  ..., -0.4352, -0.4937, -1.1182],\n",
       "          ...,\n",
       "          [ 0.3479,  1.6038, -0.6712,  ...,  0.1415, -0.7793, -1.4379],\n",
       "          [-0.1185,  0.1389, -0.3428,  ..., -1.3169, -0.4155,  0.8063],\n",
       "          [-1.9981, -0.5572, -2.4026,  ...,  1.8289,  0.9017, -0.1515]],\n",
       "\n",
       "         [[-0.1984,  0.0052, -0.0089,  ...,  0.2055, -0.7555, -0.0283],\n",
       "          [ 0.4654,  0.0470, -0.6714,  ..., -1.5033, -1.3525,  1.7332],\n",
       "          [-0.8985, -2.0968,  1.1491,  ...,  0.4295, -0.4131,  1.3679],\n",
       "          ...,\n",
       "          [-0.5210,  0.8376,  1.2637,  ..., -0.9215, -0.8712, -0.7195],\n",
       "          [-1.0403,  0.8297,  0.6137,  ...,  1.9865, -0.3440, -0.6265],\n",
       "          [-0.4667, -0.6754, -0.8490,  ...,  0.6578, -0.7851, -2.1085]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_samples['lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model,(samples,extra_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 0: torch.Size([768, 256])\n",
      "Parameter 1: torch.Size([768])\n",
      "Parameter 2: torch.Size([256, 256])\n",
      "Parameter 3: torch.Size([256])\n",
      "Parameter 4: torch.Size([2048, 256])\n",
      "Parameter 5: torch.Size([2048])\n",
      "Parameter 6: torch.Size([256, 2048])\n",
      "Parameter 7: torch.Size([256])\n",
      "Parameter 8: torch.Size([256])\n",
      "Parameter 9: torch.Size([256])\n",
      "Parameter 10: torch.Size([256])\n",
      "Parameter 11: torch.Size([256])\n",
      "Parameter 12: torch.Size([768, 256])\n",
      "Parameter 13: torch.Size([768])\n",
      "Parameter 14: torch.Size([256, 256])\n",
      "Parameter 15: torch.Size([256])\n",
      "Parameter 16: torch.Size([2048, 256])\n",
      "Parameter 17: torch.Size([2048])\n",
      "Parameter 18: torch.Size([256, 2048])\n",
      "Parameter 19: torch.Size([256])\n",
      "Parameter 20: torch.Size([256])\n",
      "Parameter 21: torch.Size([256])\n",
      "Parameter 22: torch.Size([256])\n",
      "Parameter 23: torch.Size([256])\n",
      "Parameter 24: torch.Size([768, 256])\n",
      "Parameter 25: torch.Size([768])\n",
      "Parameter 26: torch.Size([256, 256])\n",
      "Parameter 27: torch.Size([256])\n",
      "Parameter 28: torch.Size([2048, 256])\n",
      "Parameter 29: torch.Size([2048])\n",
      "Parameter 30: torch.Size([256, 2048])\n",
      "Parameter 31: torch.Size([256])\n",
      "Parameter 32: torch.Size([256])\n",
      "Parameter 33: torch.Size([256])\n",
      "Parameter 34: torch.Size([256])\n",
      "Parameter 35: torch.Size([256])\n",
      "Parameter 36: torch.Size([768, 256])\n",
      "Parameter 37: torch.Size([768])\n",
      "Parameter 38: torch.Size([256, 256])\n",
      "Parameter 39: torch.Size([256])\n",
      "Parameter 40: torch.Size([2048, 256])\n",
      "Parameter 41: torch.Size([2048])\n",
      "Parameter 42: torch.Size([256, 2048])\n",
      "Parameter 43: torch.Size([256])\n",
      "Parameter 44: torch.Size([256])\n",
      "Parameter 45: torch.Size([256])\n",
      "Parameter 46: torch.Size([256])\n",
      "Parameter 47: torch.Size([256])\n",
      "Parameter 48: torch.Size([768, 256])\n",
      "Parameter 49: torch.Size([768])\n",
      "Parameter 50: torch.Size([256, 256])\n",
      "Parameter 51: torch.Size([256])\n",
      "Parameter 52: torch.Size([2048, 256])\n",
      "Parameter 53: torch.Size([2048])\n",
      "Parameter 54: torch.Size([256, 2048])\n",
      "Parameter 55: torch.Size([256])\n",
      "Parameter 56: torch.Size([256])\n",
      "Parameter 57: torch.Size([256])\n",
      "Parameter 58: torch.Size([256])\n",
      "Parameter 59: torch.Size([256])\n",
      "Parameter 60: torch.Size([768, 256])\n",
      "Parameter 61: torch.Size([768])\n",
      "Parameter 62: torch.Size([256, 256])\n",
      "Parameter 63: torch.Size([256])\n",
      "Parameter 64: torch.Size([2048, 256])\n",
      "Parameter 65: torch.Size([2048])\n",
      "Parameter 66: torch.Size([256, 2048])\n",
      "Parameter 67: torch.Size([256])\n",
      "Parameter 68: torch.Size([256])\n",
      "Parameter 69: torch.Size([256])\n",
      "Parameter 70: torch.Size([256])\n",
      "Parameter 71: torch.Size([256])\n",
      "Parameter 72: torch.Size([3, 256])\n",
      "Parameter 73: torch.Size([3])\n",
      "Parameter 74: torch.Size([3, 256])\n",
      "Parameter 75: torch.Size([3])\n",
      "Parameter 76: torch.Size([3, 256])\n",
      "Parameter 77: torch.Size([3])\n",
      "Parameter 78: torch.Size([1, 256])\n",
      "Parameter 79: torch.Size([1])\n",
      "Parameter 80: torch.Size([1, 256])\n",
      "Parameter 81: torch.Size([1])\n",
      "Parameter 82: torch.Size([1, 256])\n",
      "Parameter 83: torch.Size([1])\n",
      "Parameter 84: torch.Size([3, 256])\n",
      "Parameter 85: torch.Size([256, 6])\n",
      "Parameter 86: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(params):\n",
    "    print(f\"Parameter {i}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 학습 가능한 파라미터 수: 7896076\n"
     ]
    }
   ],
   "source": [
    "print(f\"총 학습 가능한 파라미터 수: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31653132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
